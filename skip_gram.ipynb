{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8ab15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421c416f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9836bf",
   "metadata": {},
   "source": [
    "### Function to save and load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449664ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(name, obj):\n",
    "    \"\"\"\n",
    "    Function to save an object as pickle file\n",
    "    \"\"\"\n",
    "    with open(name, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "\n",
    "def load_file(name):\n",
    "    \"\"\"\n",
    "    Function to load a pickle object\n",
    "    \"\"\"\n",
    "    return pickle.load(open(name, \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aac2d5",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effc0c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_path = \"Output/tokens.pkl\"\n",
    "file_path = \"Input/complaints.csv\"\n",
    "col_name = \"Consumer complaint narrative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7175fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd148f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25307207",
   "metadata": {},
   "source": [
    "### Drop missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfb16db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset=[col_name], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9057392c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd02f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = data[col_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a403a0a",
   "metadata": {},
   "source": [
    "### Convert text to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a713509",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = [i.lower() for i in tqdm(input_text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffb923d",
   "metadata": {},
   "source": [
    "### Remove punctuations except apostrophe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79d3e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = [re.sub(r\"[^\\w\\d'\\s]+\", \" \", i) for i in tqdm(input_text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e509c438",
   "metadata": {},
   "source": [
    "### Remove digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1f4119",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = [re.sub(\"\\d+\", \"\", i) for i in tqdm(input_text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fcb906",
   "metadata": {},
   "source": [
    "### Remove 'xxxx' in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c2ae6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = [re.sub(r'[x]{2,}', \"\", i) for i in tqdm(input_text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436ba609",
   "metadata": {},
   "source": [
    "### Remove additional spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd85611",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = [re.sub(' +', ' ', i) for i in tqdm(input_text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b3eb14",
   "metadata": {},
   "source": [
    "### Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24308af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [word_tokenize(t) for t in tqdm(input_text[:100])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e39be9f",
   "metadata": {},
   "source": [
    "### Save tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f07653",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file(tokens_path, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a385d2",
   "metadata": {},
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb68b742",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "t = 1e-5\n",
    "context_window = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fab0f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, input_data, context_window=5, out_path=\"Output\",\n",
    "                 t=1e-5, k=10):\n",
    "        # Get word count\n",
    "        self.k = k\n",
    "        self.context_window = context_window\n",
    "        print(\"Counting word tokens...\")\n",
    "        counter = Counter([t for d in tqdm(input_data) for t in d])\n",
    "        self.vocab_count = len(counter)\n",
    "        print(f\"Unique words in the corpus: {self.vocab_count}\")\n",
    "        print(\"Creating data samples...\")\n",
    "        self.samples = self.positive_samples(input_data)\n",
    "        word2idx = dict()\n",
    "        idx2word = dict()\n",
    "        sampling_prob = []\n",
    "        print(\"Generating vocabulary...\")\n",
    "        for i, c in enumerate(counter.most_common(len(counter))):\n",
    "            word2idx[c[0]] = i\n",
    "            idx2word[i] = c[0]\n",
    "            sampling_prob.append(c[1])\n",
    "        self.word2idx = word2idx\n",
    "        self.idx2word = idx2word\n",
    "        print(\"Calculating sampling probabilities...\")\n",
    "        sampling_prob = np.sqrt(t/np.array(sampling_prob))\n",
    "        sampling_prob = sampling_prob / np.sum(sampling_prob)\n",
    "        self.sampling_prob = sampling_prob\n",
    "        print(\"Saving files...\")\n",
    "        self.save_files(out_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.samples.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        neg_words = self.negative_samples()\n",
    "        center_word = self.word2idx[self.samples.loc[idx, \"center_word\"]]\n",
    "        context_word = self.word2idx[self.samples.loc[idx, \"context_word\"]]\n",
    "        return torch.tensor(center_word), torch.tensor([context_word]+neg_words)\n",
    "\n",
    "    def positive_samples(self, input_data):\n",
    "        samples = []\n",
    "        cw = self.context_window\n",
    "        for data in tqdm(input_data):\n",
    "            text = [None] * cw + data + [None] * cw\n",
    "            for i in range(cw, len(text) - cw):\n",
    "                samples.append((text[i], text[i - cw:i] + text[i + 1: i + cw + 1]))\n",
    "        samples = pd.DataFrame(samples, columns=[\"center_word\", \"context_word\"])\n",
    "        samples = samples.explode(\"context_word\")\n",
    "        samples.dropna(inplace=True)\n",
    "        samples.reset_index(drop=True, inplace=True)\n",
    "        return samples\n",
    "\n",
    "    def negative_samples(self):\n",
    "        neg_words = list(np.random.choice(np.arange(self.vocab_count), self.k,\n",
    "                                          p=self.sampling_prob))\n",
    "        return neg_words\n",
    "\n",
    "    def save_files(self, out_path=\"Output\"):\n",
    "        save_file(os.path.join(out_path, \"word2idx.pkl\"), self.word2idx)\n",
    "        save_file(os.path.join(out_path, \"idx2word.pkl\"), self.idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a0d15b",
   "metadata": {},
   "source": [
    "# Skip-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14303c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bc56c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_len, embedding_size=64):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_len, embedding_size)\n",
    "        self.weights = torch.empty(embedding_size, vocab_len, requires_grad=True).type(torch.FloatTensor)\n",
    "        _ = torch.nn.init.normal_(self.weights)\n",
    "        self.out = nn.LogSigmoid()\n",
    "\n",
    "    def forward(self, center_word, context_words):\n",
    "        embeddings_ = self.embeddings(center_word)\n",
    "        weights_ = self.weights[:, context_words]\n",
    "        output = torch.einsum('bi,ibo->bo', embeddings_, weights_)\n",
    "        true_y = torch.zeros(output.shape[0], dtype=torch.int64)\n",
    "        return self.out(output), true_y\n",
    "\n",
    "    def save_files(self, out_path=\"Output\"):\n",
    "        save_file(os.path.join(out_path, \"emb.pkl\"), self.embeddings)\n",
    "        save_file(os.path.join(out_path, \"weights.pkl\"), self.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d743f47d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3f1c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "lr = 0.01\n",
    "num_epochs = 2\n",
    "batch_size = 128\n",
    "context_window = 5\n",
    "out_path = \"Output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43bcf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c08694b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sg(dataloader, model, criterion, optimizer, device, num_epochs):\n",
    "    model.train()\n",
    "    best_loss = 1e8\n",
    "    patience = 0\n",
    "    for i in range(num_epochs):\n",
    "        epoch_loss = []\n",
    "        print(f\"Epoch {i+1} of {num_epochs}\")\n",
    "        for center_word, context_words in tqdm(dataloader):\n",
    "            center_word = center_word.to(device)\n",
    "            context_words = context_words.to(device)\n",
    "            output, true_y = model(center_word, context_words)\n",
    "            loss = criterion(output, true_y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss.append(loss.item())\n",
    "        epoch_loss = np.mean(epoch_loss)\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "        print(f\"Loss: {epoch_loss}\")\n",
    "        if patience == 5:\n",
    "            print(\"Early stopping...\")\n",
    "    model.save_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff902a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SkipGramDataset(input_data=tokens, \n",
    "                          context_window=context_window,\n",
    "                          out_path=out_path, \n",
    "                          t=t, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f1e47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, \n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=True, \n",
    "                                         drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad0ea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SkipGram(dataset.vocab_count, embedding_size=embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27590d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12580818",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sg(dataloader, model, criterion, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1d5dd1",
   "metadata": {},
   "source": [
    "# Using embedings to get word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97c9fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = load_file(\"Output/word2idx.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f220a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx[\"payments\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945905c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = load_file(\"Output/emb.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580df0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings(torch.tensor(83))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed477fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
